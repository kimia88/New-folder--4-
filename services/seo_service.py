import json
import logging
import re
import time
from typing import List, Dict, Any, Optional, Tuple

from sentence_transformers import util
from services.seo_title_evaluator import SEOTitleEvaluator


class SEOServiceAdvanced:
    def __init__(self, db, q_service, min_score: float = 8.5, retries: int = 8, delay: float = 4.0, max_backoff: float = 60.0):
        self.db = db
        self.q_service = q_service
        self.base_min_score = min_score
        self.retries = retries
        self.delay = delay
        self.max_backoff = max_backoff
        self.evaluator = SEOTitleEvaluator()
        self.past_scores: List[float] = []

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s | %(levelname)s | %(message)s",
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler("seo_service_advanced.log", encoding="utf-8")
            ]
        )

    def extract_focus_keyword(self, title: str) -> str:
        words = re.findall(r'\b\w+\b', title.lower())
        stopwords = set(["Ø¨Ø±Ø§ÛŒ", "Ø§Ø²", "Ø¯Ø±", "Ø¨Ø§", "Ø¨Ù‡", "Ùˆ", "ÛŒØ§", "Ø§ÛŒÙ†", "Ú©Ù‡", "Ø±Ø§", "ÛŒÚ©", "ØªØ§", "Ø¢Ù†", "Ù…ÛŒ"])  # ÙØ§Ø±Ø³ÛŒ
        keywords = [w for w in words if w not in stopwords and len(w) > 2]
        return " ".join(sorted(keywords, key=len, reverse=True)[:5])

    def _semantic_similarity(self, a: str, b: str) -> float:
        try:
            emb_a = self.evaluator.model.encode(a, convert_to_tensor=True, normalize_embeddings=True)
            emb_b = self.evaluator.model.encode(b, convert_to_tensor=True, normalize_embeddings=True)
            return util.cos_sim(emb_a, emb_b).item()
        except Exception as e:
            logging.warning("Semantic similarity failed: %s", e)
            return 0.0

    def _looks_gibberish(self, title: str) -> bool:
        words = re.findall(r'\w+', title)
        gibberish_count = sum(1 for w in words if len(w) > 20 or not re.search(r'[a-zA-Z0-9Ø€-Û¿]', w))
        return gibberish_count / len(words) > 0.3 if words else False

    def analyze_weaknesses(self, title: str, keyword: str) -> List[str]:
        weaknesses = []
        title_lower = title.lower()
        keyword_lower = keyword.lower()

        if not title_lower.startswith(keyword_lower):
            weaknesses.append("Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ø¨ØªØ¯Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§Ø´Ø¯")

        if len(title) < 15:
            weaknesses.append("Ø¹Ù†ÙˆØ§Ù† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª")
        elif len(title) > 60:
            weaknesses.append("Ø¹Ù†ÙˆØ§Ù† Ø®ÛŒÙ„ÛŒ Ø¨Ù„Ù†Ø¯ Ø§Ø³Øª")

        count_focus = title_lower.count(keyword_lower)
        if count_focus / max(len(title.split()), 1) > 0.3:
            weaknesses.append("Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ ØªÚ©Ø±Ø§Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª")

        if title and not title[0].isupper():
            weaknesses.append("Ø­Ø±Ù Ø§ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ÛŒØ¯ Ø¨Ø²Ø±Ú¯ Ø¨Ø§Ø´Ø¯")

        if not any(p in title for p in [":", "-", "?"]):
            weaknesses.append("Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø§Ø² Ù†Ø´Ø§Ù†Ù‡ Ù†Ú¯Ø§Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯")

        if self._looks_gibberish(title):
            weaknesses.append("Ø¹Ù†ÙˆØ§Ù† Ø¨Ù‡ Ù†Ø¸Ø± Ù†Ø§Ù…ÙÙ‡ÙˆÙ… ÛŒØ§ gibberish Ø§Ø³Øª")

        return weaknesses

    def improve_title(self, title: str, keyword: str, weaknesses: List[str]) -> Tuple[str, float, float]:
     original_score = self.evaluator.evaluate_title(title, keyword)
     keyword_lower = keyword.lower()

    # Ø¨Ù‡Ø¨ÙˆØ¯Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¶Ø¹Ùâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµâ€ŒØ´Ø¯Ù‡
     if "Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ø¨ØªØ¯Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§Ø´Ø¯" in weaknesses:
        if not title.lower().startswith(keyword_lower):
            # Ø­Ø°Ù Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø¬Ø§ÛŒ Ø¯ÛŒÚ¯Ù‡
            title_no_kw = re.sub(re.escape(keyword), '', title, flags=re.IGNORECASE).strip()
            title = f"{keyword} {title_no_kw}"

     if "Ø¹Ù†ÙˆØ§Ù† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª" in weaknesses:
        if len(title) < 20:
            title += " - Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ"

     if "Ø¹Ù†ÙˆØ§Ù† Ø®ÛŒÙ„ÛŒ Ø¨Ù„Ù†Ø¯ Ø§Ø³Øª" in weaknesses:
        if len(title) > 60:
            title = title[:57].rstrip() + "..."

     if "Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ ØªÚ©Ø±Ø§Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª" in weaknesses:
        words = title.split()
        count_kw = sum(1 for w in words if w.lower() == keyword_lower)
        if count_kw / max(len(words), 1) > 0.3:
            words = [w for w in words if w.lower() != keyword_lower]
            title = f"{keyword} {' '.join(words)}"

     if "Ø­Ø±Ù Ø§ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ÛŒØ¯ Ø¨Ø²Ø±Ú¯ Ø¨Ø§Ø´Ø¯" in weaknesses:
        if title and not title[0].isupper():
            title = title[0].upper() + title[1:]

     if "Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø§Ø² Ù†Ø´Ø§Ù†Ù‡ Ù†Ú¯Ø§Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯" in weaknesses:
        if not any(p in title for p in [":", "-", "ØŸ", "Ø›"]):
            title += " - Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ù…Ù„"

     if "Ø¹Ù†ÙˆØ§Ù† Ø¨Ù‡ Ù†Ø¸Ø± Ù†Ø§Ù…ÙÙ‡ÙˆÙ… ÛŒØ§ gibberish Ø§Ø³Øª" in weaknesses:
        title = f"{keyword} - Ø³Ø§Ø¯Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„â€ŒÙÙ‡Ù… Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡"

    # Ù†Ù…Ø±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
     new_score = self.evaluator.evaluate_title(title, keyword)
     return title, original_score, new_score


    def _parse_response(self, raw: str) -> Dict[str, Any]:
        try:
            match = re.search(r'\{.*?"original_title".*?"score"\s*:\s*\d+.*?\}', raw, re.DOTALL)
            if match:
                return json.loads(match.group(0))
            raise ValueError("JSON block not found.")
        except Exception as e:
            logging.debug("Failed to parse response: %s | Raw: %s", e, raw)
            raise

    def _build_prompt(self, original: str, lang_id: int, emphasize_change: bool, previous: Optional[str] = None) -> str:
        examples_fa = (
            'Ù…Ø«Ø§Ù„:\n'
            '{\n'
            '  "original_title": "Ú†Ø±Ø§ Ø®ÙˆØ§Ø¨ Ú©Ø§ÙÛŒ Ù…Ù‡Ù… Ø§Ø³ØªØŸ",\n'
            '  "optimized_title": "Ø®ÙˆØ§Ø¨ Ú©Ø§ÙÛŒØ› Ú©Ù„ÛŒØ¯ Ø·Ù„Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ù„Ø§Ù…Øª Ø°Ù‡Ù† Ùˆ Ø¨Ø¯Ù†",\n'
            '  "score": 9.2\n'
            '}\n\n'
            '{\n'
            '  "original_title": "ØªØ±ÙÙ†Ø¯Ù‡Ø§ÛŒ Ø¹Ú©Ø§Ø³ÛŒ Ø¨Ø§ Ù…ÙˆØ¨Ø§ÛŒÙ„",\n'
            '  "optimized_title": "Û±Û° ØªØ±ÙÙ†Ø¯ Ø·Ù„Ø§ÛŒÛŒ Ø¹Ú©Ø§Ø³ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Ù…ÙˆØ¨Ø§ÛŒÙ„",\n'
            '  "score": 9.0\n'
            '}\n\n'
        )

        prompt = (
            "Ø¹Ù†ÙˆØ§Ù† Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ SEO Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†. ÙÙ‚Ø· Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ JSON Ø²ÛŒØ± ØªÙˆÙ„ÛŒØ¯ Ú©Ù†:\n"
            '{\n  "original_title": "...",\n  "optimized_title": "...",\n  "score": Ø¹Ø¯Ø¯ÛŒ Ø¨ÛŒÙ† Û° ØªØ§ Û±Û°\n}\n\n'
            + examples_fa +
            f"Ø¹Ù†ÙˆØ§Ù†:\n{original.strip()}\n"
        )

        if previous:
            prompt += f"\nØ¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ù‚Ø¨Ù„ÛŒ:\n{previous}"

        if emphasize_change:
            prompt += (
                "\nâ— Ù„Ø·ÙØ§Ù‹ Ù†Ø³Ø®Ù‡â€ŒØ§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ùˆ ØªØ§Ø«ÛŒØ±Ú¯Ø°Ø§Ø± Ø¨Ø§ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¨Ø¯Ù‡."
                "\nğŸ” Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§ØªÛŒ Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø§Ù„Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù† Ùˆ Ø¬Ø°Ø§Ø¨ÛŒØª Ø§Ù†Ø³Ø§Ù†ÛŒ Ùˆ Ø³Ø¦Ùˆ Ø±Ø§ Ù‡Ù…Ø²Ù…Ø§Ù† Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±."
            )
        else:
            prompt += "\nâœ… ÙÙ‚Ø· Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù…Ù„Ø§ÛŒÙ… Ø¨Ø§ Ø­ÙØ¸ Ù…Ø¹Ù†Ø§ÛŒ Ø§ØµÙ„ÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ."

        return prompt

    def _attempt_optimization(self, title: str, keyword: str, lang_id: int, original_score: float) -> List[Dict[str, Any]]:
        results = []
        backoff = self.delay
        previous_title = None

        focus_areas = {
            "keyword": False,
            "creativity": False,
            "grammar": False,
            "semantic": False,
        }

        for attempt in range(1, self.retries + 1):
            emphasize_parts = []

            # ØªØ­Ù„ÛŒÙ„ Ø¶Ø¹Ùâ€ŒÙ‡Ø§ Ø¨Ø§ ØªØ§Ø¨Ø¹ analyze_weaknesses Ø¨Ø±Ø§ÛŒ ØªØ§Ú©ÛŒØ¯ Ù¾ÙˆÛŒØ§
            weaknesses = self.analyze_weaknesses(title, keyword)
            if original_score < self.base_min_score:
                if "Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ø¨ØªØ¯Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§Ø´Ø¯" in weaknesses:
                    focus_areas["keyword"] = True
                if "Ø¹Ù†ÙˆØ§Ù† Ø¨Ù‡ Ù†Ø¸Ø± Ù†Ø§Ù…ÙÙ‡ÙˆÙ… ÛŒØ§ gibberish Ø§Ø³Øª" in weaknesses:
                    focus_areas["creativity"] = True
                # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ú¯Ø±Ø§Ù…Ø± Ø±Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒ Ù…Ø«Ù„Ø§Ù‹:
                # if some_grammar_check(title) == False:
                #    focus_areas["grammar"] = True
                if original_score < self.base_min_score / 2:
                    focus_areas = {k: True for k in focus_areas}

            emphasize_change = any(focus_areas.values())

            prompt = self._build_prompt(title, lang_id, emphasize_change, previous=previous_title)

            if emphasize_change:
                if focus_areas["keyword"]:
                    emphasize_parts.append("ØªÙ…Ø±Ú©Ø² Ø±ÙˆÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ùˆ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ø² Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§Ø´Ø¯.")
                if focus_areas["creativity"]:
                    emphasize_parts.append("Ø®Ù„Ø§Ù‚ÛŒØª Ø¨ÛŒØ´ØªØ± Ùˆ Ø¯ÙˆØ±ÛŒ Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ù†Ø§Ù…ÙÙ‡ÙˆÙ… Ùˆ Ø¨ÛŒâ€ŒØ±Ø¨Ø·.")
                if focus_areas["grammar"]:
                    emphasize_parts.append("Ø§ØµÙ„Ø§Ø­ Ú¯Ø±Ø§Ù…Ø± Ùˆ Ù†Ú¯Ø§Ø±Ø´ ØµØ­ÛŒØ­.")
                if focus_areas["semantic"]:
                    emphasize_parts.append("Ø§Ø±ØªØ¨Ø§Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ù‚ÙˆÛŒâ€ŒØªØ± Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹.")

                prompt += "\nğŸ” Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ Ù†Ú©Ø§Øª Ø²ÛŒØ± ØªÙˆØ¬Ù‡ ÙˆÛŒÚ˜Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´:\n" + "\n".join(f"- {p}" for p in emphasize_parts)

            logging.info(f"ğŸ”„ Attempt {attempt} | Prompt sent with emphasis on: {emphasize_parts}")

            try:
                self.q_service.send_request(prompt)
                time.sleep(1.0)
                raw = self.q_service.get_response()
                if not raw or "âš ï¸" in raw:
                    raise ValueError("Invalid response")

                data = self._parse_response(raw)
                candidate = data["optimized_title"].strip()
                if not candidate or self._looks_gibberish(candidate):
                    raise ValueError("Gibberish title")

                score = self.evaluator.evaluate_title(candidate, keyword)
                similarity = self._semantic_similarity(title, candidate)

                results.append({
                    "title": candidate,
                    "score": score,
                    "similarity": similarity
                })

                logging.info(f"âœ… Attempt {attempt} | Score: {score:.2f} | Similarity: {similarity:.2f}")
                previous_title = candidate

                # Ø¨Ù‡Ø¨ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø§Ø¯Ù‡ Ø¨Ø§ ØªØ§Ø¨Ø¹ improve_title
                improved_title, _, improved_score = self.improve_title(candidate, keyword, weaknesses)
                if improved_score > score:
                    candidate = improved_title
                    score = improved_score

                if score > original_score:
                    original_score = score
                    title = candidate
                    # Ù…Ø¬Ø¯Ø¯ ØªØ­Ù„ÛŒÙ„ Ø¶Ø¹Ù Ø¨Ø±Ø§ÛŒ Ø¯ÙˆØ± Ø¨Ø¹Ø¯
                    weaknesses = self.analyze_weaknesses(title, keyword)
                    focus_areas = {
                        "keyword": "Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ø¨ØªØ¯Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§Ø´Ø¯" in weaknesses,
                        "creativity": "Ø¹Ù†ÙˆØ§Ù† Ø¨Ù‡ Ù†Ø¸Ø± Ù†Ø§Ù…ÙÙ‡ÙˆÙ… ÛŒØ§ gibberish Ø§Ø³Øª" in weaknesses,
                        "grammar": False,
                        "semantic": False,
                    }

            except Exception as e:
                logging.warning(f"âš ï¸ Attempt {attempt} failed: {e}")

            time.sleep(backoff)
            backoff = min(backoff * 2, self.max_backoff)

        return results

    def _select_best_attempt(self, original_score: float, original_title: str, attempts: List[Dict[str, Any]]) -> Tuple[str, float]:
        if not attempts:
            return original_title, original_score
        best = max(attempts, key=lambda x: x["score"])
        if best["score"] > original_score:
            return best["title"], best["score"]
        return original_title, original_score

    def _process_single_content(self, content: Dict[str, Any]) -> Optional[Dict[str, Any]]:
     content_id = content.get("content_id") or content.get("ContentID")
     title = (content.get("title") or content.get("Title") or "").strip()

     if not title:
        logging.info("âŒ Skipped empty title for content_id %s", content_id)
        return None

     logging.info("â–¶ï¸ Processing content_id %s | Title: %s", content_id, title)

     keyword = self.extract_focus_keyword(title)
     original_score = self.evaluator.evaluate_title(title, keyword)
     self.past_scores.append(original_score)

     attempts = self._attempt_optimization(title, keyword, lang_id=0, original_score=original_score)
     best_title, best_score = self._select_best_attempt(original_score, title, attempts)

     logging.info(f"âœ”ï¸ Best optimized title score: {best_score:.2f} (original was {original_score:.2f})")

     return {
        "content_id": content_id,
        "original_title": title,
        "optimized_title": best_title,
        "score": best_score
    }


class SEOTitleEvaluator:
    def __init__(self):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    def evaluate_title(self, title: str, keyword: str) -> float:
        score = 0.0
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ù‡ Ù†Ø³Ø¨Øª Ø·ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù†
        score += min(len(title) / 15.0, 2.0) * 2.0
        # Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ: Ú†Ù†Ø¯ Ø¨Ø§Ø± Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¯Ø§Ø®Ù„ Ø¹Ù†ÙˆØ§Ù† Ø§Ø³Øª
        keyword_count = title.lower().count(keyword.lower())
        score += min(keyword_count, 3) * 2.0
        # Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±
        meaningful_words = len([w for w in re.findall(r'\b\w+\b', title) if len(w) > 2])
        score += min(meaningful_words / 7.0, 2.0) * 2.0

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙØ§Ú©ØªÙˆØ± Ù…Ø¹Ù†Ø§ÛŒÛŒ (Ù…Ø«Ù„Ø§Ù‹ ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ø§ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ)
        try:
            emb_title = self.model.encode(title, convert_to_tensor=True, normalize_embeddings=True)
            emb_keyword = self.model.encode(keyword, convert_to_tensor=True, normalize_embeddings=True)
            semantic_score = util.cos_sim(emb_title, emb_keyword).item()
            score += semantic_score * 4.0  # ÙˆØ²Ù† Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒØ´ØªØ±
        except Exception as e:
            logging.warning("Semantic evaluation failed: %s", e)

        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
        return min(score, 10.0)
